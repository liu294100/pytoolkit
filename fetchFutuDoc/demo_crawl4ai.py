#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Crawl4AI å¯Œé€”æ–‡æ¡£çˆ¬è™«æ¼”ç¤ºè„šæœ¬
Crawl4AI Futu Document Crawler Demo Script

è¿™ä¸ªè„šæœ¬æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨æ–°çš„ Crawl4AI æ¡†æ¶æ¥çˆ¬å–å¯Œé€”ç‰›ç‰›å¸®åŠ©æ–‡æ¡£

è¿è¡Œæ–¹å¼:
1. åŸºç¡€æ¼”ç¤º: python demo_crawl4ai.py
2. è‡ªå®šä¹‰URL: python demo_crawl4ai.py --urls https://support.futunn.com/categories/2186
3. é«˜å¹¶å‘æ¨¡å¼: python demo_crawl4ai.py --max-concurrent 10
"""

import asyncio
import argparse
import sys
import os
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

try:
    from src.core.crawl4ai_crawler import (
        Crawl4AIFutuCrawler,
        Crawl4AISettings,
        run_crawl4ai_crawler
    )
except ImportError:
    print("âŒ æ— æ³•å¯¼å…¥ Crawl4AI çˆ¬è™«æ¨¡å—")
    print("è¯·ç¡®ä¿å·²å®‰è£…ä¾èµ–: pip install crawl4ai aiohttp aiofiles")
    sys.exit(1)

async def demo_basic_crawl():
    """åŸºç¡€çˆ¬å–æ¼”ç¤º"""
    print("ğŸš€ Crawl4AI åŸºç¡€çˆ¬å–æ¼”ç¤º")
    print("=" * 50)
    
    # æ¼”ç¤ºURLï¼ˆç®€ä½“ä¸­æ–‡ç‰ˆæœ¬ï¼‰
    demo_urls = [
        "https://support.futunn.com/categories/2186",  # åŸºç¡€çŸ¥è¯†å…¥é—¨
        "https://support.futunn.com/categories/2187",  # å®¢æˆ·ç«¯åŠŸèƒ½
    ]
    
    print(f"ğŸ“‹ æ¼”ç¤ºURLæ•°é‡: {len(demo_urls)}")
    for i, url in enumerate(demo_urls, 1):
        print(f"  {i}. {url}")
    
    # åˆ›å»ºè®¾ç½®
    settings = Crawl4AISettings(
        max_concurrent=3,
        delay_range=(1.0, 2.0),
        timeout=30,
        output_dir='demo_output',
        headless=True,
        enable_js=True
    )
    
    print(f"\nâš™ï¸ çˆ¬å–è®¾ç½®:")
    print(f"  æœ€å¤§å¹¶å‘: {settings.max_concurrent}")
    print(f"  å»¶è¿ŸèŒƒå›´: {settings.delay_range}")
    print(f"  è¶…æ—¶æ—¶é—´: {settings.timeout}ç§’")
    print(f"  è¾“å‡ºç›®å½•: {settings.output_dir}")
    print(f"  æ— å¤´æ¨¡å¼: {settings.headless}")
    print(f"  å¯ç”¨JS: {settings.enable_js}")
    
    print("\nğŸ”„ å¼€å§‹çˆ¬å–...")
    
    try:
        # è¿è¡Œçˆ¬å–
        results, info = await run_crawl4ai_crawler(
            urls=demo_urls,
            max_depth=2,
            settings=settings
        )
        
        # æ˜¾ç¤ºç»“æœ
        stats = info['stats']
        saved_files = info['saved_files']
        
        print("\n" + "=" * 50)
        print("ğŸ‰ çˆ¬å–å®Œæˆï¼")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  æ€»å¤„ç†é¡µé¢: {stats['total_processed']}")
        print(f"  æˆåŠŸçˆ¬å–: {stats['successful_crawls']}")
        print(f"  å¤±è´¥çˆ¬å–: {stats['failed_crawls']}")
        print(f"  å”¯ä¸€é¡µé¢: {stats['unique_pages']}")
        print(f"  æ€»å†…å®¹é•¿åº¦: {stats['total_content_length']:,} å­—ç¬¦")
        
        if stats['start_time'] and stats['end_time']:
            duration = stats['end_time'] - stats['start_time']
            print(f"  è€—æ—¶: {duration}")
        
        print(f"\nğŸ“ ä¿å­˜çš„æ–‡ä»¶:")
        for lang, file_info in saved_files.items():
            print(f"  ğŸ“„ {lang.upper()}: {file_info['count']} ç¯‡æ–‡æ¡£")
            print(f"    JSON: {Path(file_info['json']).name}")
            print(f"    Markdown: {Path(file_info['markdown']).name}")
        
        # æ˜¾ç¤ºéƒ¨åˆ†ç»“æœå†…å®¹
        print(f"\nğŸ“ å†…å®¹é¢„è§ˆ:")
        successful_results = [r for r in results if r.success]
        for i, result in enumerate(successful_results[:3], 1):
            print(f"\n  {i}. {result.title}")
            print(f"     URL: {result.url}")
            print(f"     è¯­è¨€: {result.language}")
            print(f"     å†…å®¹é•¿åº¦: {len(result.content)} å­—ç¬¦")
            print(f"     é“¾æ¥æ•°é‡: {len(result.links)}")
            if result.content:
                preview = result.content[:200].replace('\n', ' ')
                print(f"     å†…å®¹é¢„è§ˆ: {preview}...")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ çˆ¬å–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def demo_custom_crawl(urls, max_concurrent=5, max_depth=2):
    """è‡ªå®šä¹‰çˆ¬å–æ¼”ç¤º"""
    print("ğŸ¯ Crawl4AI è‡ªå®šä¹‰çˆ¬å–æ¼”ç¤º")
    print("=" * 50)
    
    print(f"ğŸ“‹ è‡ªå®šä¹‰URLæ•°é‡: {len(urls)}")
    for i, url in enumerate(urls, 1):
        print(f"  {i}. {url}")
    
    # åˆ›å»ºé«˜æ€§èƒ½è®¾ç½®
    settings = Crawl4AISettings(
        max_concurrent=max_concurrent,
        delay_range=(0.5, 1.0),  # æ›´çŸ­çš„å»¶è¿Ÿ
        timeout=45,
        output_dir='custom_output',
        headless=True,
        enable_js=True,
        screenshot=False,
        wait_for_images=False
    )
    
    print(f"\nâš™ï¸ é«˜æ€§èƒ½è®¾ç½®:")
    print(f"  æœ€å¤§å¹¶å‘: {settings.max_concurrent}")
    print(f"  æœ€å¤§æ·±åº¦: {max_depth}")
    print(f"  å»¶è¿ŸèŒƒå›´: {settings.delay_range}")
    
    print("\nğŸš€ å¼€å§‹é«˜æ€§èƒ½çˆ¬å–...")
    
    try:
        results, info = await run_crawl4ai_crawler(
            urls=urls,
            max_depth=max_depth,
            settings=settings
        )
        
        stats = info['stats']
        saved_files = info['saved_files']
        
        print("\n" + "=" * 50)
        print("ğŸ‰ è‡ªå®šä¹‰çˆ¬å–å®Œæˆï¼")
        print(f"ğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
        print(f"  æ€»å¤„ç†é¡µé¢: {stats['total_processed']}")
        print(f"  æˆåŠŸç‡: {stats['successful_crawls']}/{stats['total_processed']} ({stats['successful_crawls']/max(stats['total_processed'], 1)*100:.1f}%)")
        
        if stats['start_time'] and stats['end_time']:
            duration = stats['end_time'] - stats['start_time']
            duration_seconds = duration.total_seconds()
            if duration_seconds > 0:
                pages_per_second = stats['successful_crawls'] / duration_seconds
                print(f"  çˆ¬å–é€Ÿåº¦: {pages_per_second:.2f} é¡µé¢/ç§’")
                print(f"  å¹³å‡å“åº”æ—¶é—´: {duration_seconds/max(stats['total_processed'], 1):.2f} ç§’/é¡µé¢")
        
        # æŒ‰è¯­è¨€åˆ†ç»„ç»Ÿè®¡
        lang_stats = {}
        for result in results:
            if result.success:
                lang = result.language
                if lang not in lang_stats:
                    lang_stats[lang] = {'count': 0, 'total_length': 0}
                lang_stats[lang]['count'] += 1
                lang_stats[lang]['total_length'] += len(result.content)
        
        print(f"\nğŸŒ è¯­è¨€åˆ†å¸ƒ:")
        for lang, stats_data in lang_stats.items():
            avg_length = stats_data['total_length'] / stats_data['count']
            print(f"  {lang.upper()}: {stats_data['count']} é¡µé¢, å¹³å‡é•¿åº¦: {avg_length:.0f} å­—ç¬¦")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ è‡ªå®šä¹‰çˆ¬å–å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Crawl4AI å¯Œé€”æ–‡æ¡£çˆ¬è™«æ¼”ç¤º",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ğŸš€ æ¼”ç¤ºç¤ºä¾‹:
  åŸºç¡€æ¼”ç¤º:     python demo_crawl4ai.py
  è‡ªå®šä¹‰URL:    python demo_crawl4ai.py --urls https://support.futunn.com/categories/2186
  é«˜å¹¶å‘æ¨¡å¼:   python demo_crawl4ai.py --max-concurrent 10 --max-depth 3
  å¤šè¯­è¨€çˆ¬å–:   python demo_crawl4ai.py --urls https://support.futunn.com/categories/2186 https://support.futunn.com/en/categories/2186
        """
    )
    
    parser.add_argument(
        '--urls', 
        nargs='+', 
        help='è‡ªå®šä¹‰URLåˆ—è¡¨ï¼ˆå¦‚æœä¸æŒ‡å®šï¼Œå°†ä½¿ç”¨æ¼”ç¤ºURLï¼‰'
    )
    parser.add_argument(
        '--max-concurrent', 
        type=int, 
        default=5, 
        help='æœ€å¤§å¹¶å‘æ•° (é»˜è®¤: 5)'
    )
    parser.add_argument(
        '--max-depth', 
        type=int, 
        default=2, 
        help='æœ€å¤§çˆ¬å–æ·±åº¦ (é»˜è®¤: 2)'
    )
    parser.add_argument(
        '--demo-type', 
        choices=['basic', 'custom'], 
        default='basic',
        help='æ¼”ç¤ºç±»å‹ (é»˜è®¤: basic)'
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥ Crawl4AI å®‰è£…
    try:
        import crawl4ai
        print(f"âœ… Crawl4AI å·²å®‰è£… (ç‰ˆæœ¬: {crawl4ai.__version__ if hasattr(crawl4ai, '__version__') else 'æœªçŸ¥'})")
    except ImportError:
        print("âŒ Crawl4AI æœªå®‰è£…")
        print("è¯·è¿è¡Œ: pip install crawl4ai aiohttp aiofiles")
        sys.exit(1)
    
    print(f"ğŸ¤– Python ç‰ˆæœ¬: {sys.version}")
    print(f"ğŸ“ å·¥ä½œç›®å½•: {os.getcwd()}")
    print()
    
    # è¿è¡Œæ¼”ç¤º
    if args.urls:
        # è‡ªå®šä¹‰URLæ¼”ç¤º
        success = asyncio.run(demo_custom_crawl(
            urls=args.urls,
            max_concurrent=args.max_concurrent,
            max_depth=args.max_depth
        ))
    else:
        # åŸºç¡€æ¼”ç¤º
        success = asyncio.run(demo_basic_crawl())
    
    if success:
        print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ’¡ æç¤º:")
        print("  - æŸ¥çœ‹ç”Ÿæˆçš„è¾“å‡ºç›®å½•ä»¥è·å–å®Œæ•´ç»“æœ")
        print("  - å°è¯•ä¸åŒçš„å‚æ•°ç»„åˆä»¥ä¼˜åŒ–æ€§èƒ½")
        print("  - ä½¿ç”¨ GUI ç•Œé¢è·å¾—æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ: python main_crawl4ai.py")
    else:
        print("\nâŒ æ¼”ç¤ºå¤±è´¥")
        sys.exit(1)

if __name__ == '__main__':
    main()