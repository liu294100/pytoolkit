#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¯Œé€”ç‰›ç‰›å¸®åŠ©ä¸­å¿ƒæ·±åº¦çˆ¬è™«æ ¸å¿ƒæ¨¡å—
Futu Help Center Deep Crawler Core Module
æ”¯æŒæ·±åº¦é€’å½’çˆ¬å–åˆ†ç±»ä¸‹çš„æ‰€æœ‰æ–‡ç« é“¾æ¥

ä¼˜åŒ–ç‰¹æ€§:
- å¢å¼ºçš„HTMLè§£æç²¾åº¦
- æ™ºèƒ½å†…å®¹è´¨é‡æ£€æµ‹
- æ”¹è¿›çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
- ä¼˜åŒ–çš„å¹¶å‘æ§åˆ¶
- çµæ´»çš„é…ç½®ç®¡ç†
"""

import requests
import json
import os
import time
import re
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from datetime import datetime
from typing import Dict, List, Optional, Set, Tuple, Union
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import random
from dataclasses import dataclass, field
from pathlib import Path
import hashlib

@dataclass
class CrawlerSettings:
    """çˆ¬è™«é…ç½®è®¾ç½®"""
    max_workers: int = 3
    delay_range: Tuple[float, float] = (1.0, 3.0)
    timeout: int = 30
    retries: int = 3
    max_content_length: int = 1000000  # 1MB
    min_content_length: int = 100
    output_dir: str = 'docs_deep'
    user_agent: str = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    
@dataclass
class ContentQuality:
    """å†…å®¹è´¨é‡è¯„ä¼°ç»“æœ"""
    is_valid: bool
    score: float
    issues: List[str] = field(default_factory=list)
    
class ContentQualityChecker:
    """å†…å®¹è´¨é‡æ£€æµ‹å™¨"""
    
    @staticmethod
    def check_content_quality(content: Dict[str, str]) -> ContentQuality:
        """æ£€æŸ¥å†…å®¹è´¨é‡"""
        issues = []
        score = 100.0
        
        # æ£€æŸ¥æ ‡é¢˜
        if not content.get('title') or len(content['title'].strip()) < 5:
            issues.append('æ ‡é¢˜è¿‡çŸ­æˆ–ç¼ºå¤±')
            score -= 30
            
        # æ£€æŸ¥å†…å®¹é•¿åº¦
        content_text = content.get('content', '')
        if len(content_text) < 100:
            issues.append('å†…å®¹è¿‡çŸ­')
            score -= 40
        elif len(content_text) > 100000:
            issues.append('å†…å®¹è¿‡é•¿ï¼Œå¯èƒ½åŒ…å«å™ªéŸ³')
            score -= 20
            
        # æ£€æŸ¥å†…å®¹è´¨é‡
        if content_text:
            # æ£€æŸ¥é‡å¤å†…å®¹
            lines = content_text.split('\n')
            unique_lines = set(line.strip() for line in lines if line.strip())
            if len(lines) > 10 and len(unique_lines) / len(lines) < 0.7:
                issues.append('å†…å®¹é‡å¤åº¦è¿‡é«˜')
                score -= 15
                
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰æ„ä¹‰çš„å†…å®¹
            meaningful_chars = re.sub(r'[\s\n\r\t]+', '', content_text)
            if len(meaningful_chars) < 50:
                issues.append('æœ‰æ•ˆå†…å®¹è¿‡å°‘')
                score -= 25
                
        # æ£€æŸ¥URLæœ‰æ•ˆæ€§
        url = content.get('url', '')
        if not url or not url.startswith('http'):
            issues.append('URLæ— æ•ˆ')
            score -= 10
            
        is_valid = score >= 60 and len(issues) <= 2
        return ContentQuality(is_valid=is_valid, score=score, issues=issues)

class DeepFutuDocCrawler:
    def __init__(self, settings: Optional[CrawlerSettings] = None):
        """
        åˆå§‹åŒ–æ·±åº¦çˆ¬è™«
        
        Args:
            settings: çˆ¬è™«é…ç½®è®¾ç½®ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        self.settings = settings or CrawlerSettings()
        self.quality_checker = ContentQualityChecker()
        
        # åˆå§‹åŒ–HTTPä¼šè¯
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': self.settings.user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,zh-TW;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
            'Referer': 'https://support.futunn.com/'
        })
        
        # URLç®¡ç†
        self.visited_urls: Set[str] = set()
        self.failed_urls: Set[str] = set()
        self.url_cache: Dict[str, str] = {}  # URLå†…å®¹ç¼“å­˜
        
        # æ–‡æ¡£æ•°æ®å­˜å‚¨
        self.docs_data = {
            'zh-cn': {},  # ç®€ä½“ä¸­æ–‡
            'zh-hk': {},  # ç¹ä½“ä¸­æ–‡ï¼ˆé¦™æ¸¯ï¼‰
            'en': {}      # è‹±è¯­
        }
        
        # å¢å¼ºçš„ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_processed': 0,
            'successful_crawls': 0,
            'failed_crawls': 0,
            'categories_found': 0,
            'articles_found': 0,
            'quality_passed': 0,
            'quality_failed': 0,
            'cache_hits': 0,
            'start_time': None,
            'end_time': None
        }
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        self.logger = logging.getLogger('DeepFutuCrawler')
        self.logger.setLevel(logging.INFO)
        
        # é¿å…é‡å¤æ·»åŠ å¤„ç†å™¨
        if not self.logger.handlers:
            # æ–‡ä»¶å¤„ç†å™¨
            file_handler = logging.FileHandler('deep_crawler.log', encoding='utf-8')
            file_handler.setLevel(logging.INFO)
            
            # æ§åˆ¶å°å¤„ç†å™¨
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.INFO)
            
            # æ ¼å¼åŒ–å™¨
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            file_handler.setFormatter(formatter)
            console_handler.setFormatter(formatter)
            
            self.logger.addHandler(file_handler)
            self.logger.addHandler(console_handler)
    
    def get_page_content(self, url: str, use_cache: bool = True) -> Optional[BeautifulSoup]:
        """è·å–é¡µé¢å†…å®¹ï¼Œæ”¯æŒç¼“å­˜å’Œé‡è¯•æœºåˆ¶"""
        # æ£€æŸ¥ç¼“å­˜
        url_hash = hashlib.md5(url.encode()).hexdigest()
        if use_cache and url_hash in self.url_cache:
            self.stats['cache_hits'] += 1
            self.logger.debug(f"ä½¿ç”¨ç¼“å­˜å†…å®¹: {url}")
            return BeautifulSoup(self.url_cache[url_hash], 'html.parser')
            
        for attempt in range(self.settings.retries):
            try:
                self.logger.info(f"æ­£åœ¨çˆ¬å– (å°è¯• {attempt + 1}/{self.settings.retries}): {url}")
                
                # æ™ºèƒ½å»¶è¿Ÿç­–ç•¥
                if attempt > 0:
                    # å¤±è´¥é‡è¯•æ—¶ä½¿ç”¨æŒ‡æ•°é€€é¿
                    delay = min(2 ** attempt, 10)
                else:
                    # æ­£å¸¸è¯·æ±‚ä½¿ç”¨éšæœºå»¶è¿Ÿ
                    delay = random.uniform(*self.settings.delay_range)
                time.sleep(delay)
                
                response = self.session.get(url, timeout=self.settings.timeout)
                response.raise_for_status()
                
                # æ£€æŸ¥å“åº”å¤§å°
                if len(response.content) > self.settings.max_content_length:
                    self.logger.warning(f"å“åº”å†…å®¹è¿‡å¤§ï¼Œè·³è¿‡: {url} ({len(response.content)} bytes)")
                    return None
                    
                # æ™ºèƒ½ç¼–ç æ£€æµ‹
                if response.encoding.lower() in ['iso-8859-1', 'windows-1252'] or not response.encoding:
                    response.encoding = 'utf-8'
                    
                # éªŒè¯å†…å®¹ç±»å‹
                content_type = response.headers.get('content-type', '').lower()
                if 'text/html' not in content_type:
                    self.logger.warning(f"éHTMLå†…å®¹ï¼Œè·³è¿‡: {url} (Content-Type: {content_type})")
                    return None
                    
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ç¼“å­˜å†…å®¹
                if use_cache:
                    self.url_cache[url_hash] = response.text
                    
                self.stats['successful_crawls'] += 1
                return soup
                
            except requests.Timeout:
                self.logger.warning(f"è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1}/{self.settings.retries}) {url}")
            except requests.ConnectionError:
                self.logger.warning(f"è¿æ¥é”™è¯¯ (å°è¯• {attempt + 1}/{self.settings.retries}) {url}")
            except requests.HTTPError as e:
                if e.response.status_code == 404:
                    self.logger.warning(f"é¡µé¢ä¸å­˜åœ¨: {url}")
                    break  # 404é”™è¯¯ä¸é‡è¯•
                elif e.response.status_code in [429, 503]:
                    self.logger.warning(f"æœåŠ¡å™¨é™æµ (å°è¯• {attempt + 1}/{self.settings.retries}) {url}")
                    time.sleep(5 * (attempt + 1))  # é™æµæ—¶ç­‰å¾…æ›´é•¿æ—¶é—´
                else:
                    self.logger.warning(f"HTTPé”™è¯¯ {e.response.status_code} (å°è¯• {attempt + 1}/{self.settings.retries}) {url}")
            except Exception as e:
                self.logger.warning(f"æœªçŸ¥é”™è¯¯ (å°è¯• {attempt + 1}/{self.settings.retries}) {url}: {e}")
                
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        self.logger.error(f"æœ€ç»ˆçˆ¬å–å¤±è´¥: {url}")
        self.failed_urls.add(url)
        self.stats['failed_crawls'] += 1
        return None
    
    def extract_content_from_page(self, soup: BeautifulSoup, url: str) -> Dict[str, Union[str, List[str]]]:
        """ä»é¡µé¢æå–å†…å®¹ï¼Œå¢å¼ºçš„HTMLè§£æç²¾åº¦"""
        content = {
            'title': '',
            'content': '',
            'category': '',
            'tags': [],
            'url': url,
            'crawl_time': datetime.now().isoformat(),
            'language': self.detect_language_from_url(url),
            'meta_description': '',
            'content_hash': '',
            'word_count': 0
        }
        
        # ç§»é™¤å™ªéŸ³å…ƒç´ 
        self._remove_noise_elements(soup)
        
        # æå–æ ‡é¢˜ - å¢å¼ºçš„é€‰æ‹©å™¨
        content['title'] = self._extract_title(soup)
        
        # æå–ä¸»è¦å†…å®¹ - æ™ºèƒ½å†…å®¹æå–
        content_text = self._extract_main_content(soup, url)
        content['content'] = self.clean_text(content_text)
        content['word_count'] = len(content['content'].split())
        
        # ç”Ÿæˆå†…å®¹å“ˆå¸Œç”¨äºå»é‡
        content['content_hash'] = hashlib.md5(content['content'].encode()).hexdigest()
        
        # æå–å…ƒæ•°æ®
        content['meta_description'] = self._extract_meta_description(soup)
        content['category'] = self._extract_category(soup)
        content['tags'] = self._extract_tags(soup)
        
        return content
    
    def _remove_noise_elements(self, soup: BeautifulSoup) -> None:
        """ç§»é™¤é¡µé¢ä¸­çš„å™ªéŸ³å…ƒç´ """
        noise_selectors = [
            'script', 'style', 'nav', 'footer', 'header',
            '.advertisement', '.ads', '.sidebar', '.navigation',
            '.feedback-box', '.helpful-box', '.cai-box',
            '.social-share', '.related-articles', '.comments',
            '[data-seo-nofollow="1"]', '.breadcrumb-nav'
        ]
        
        for selector in noise_selectors:
            for element in soup.select(selector):
                element.decompose()
    
    def _extract_title(self, soup: BeautifulSoup) -> str:
        """æå–é¡µé¢æ ‡é¢˜"""
        title_selectors = [
            'h1.topic-title',  # å¯Œé€”ç‰¹å®šçš„æ ‡é¢˜é€‰æ‹©å™¨
            'h1.page-title',
            '.topic-title',
            'h1',
            '.page-title',
            '.content-title',
            '.article-title',
            'title'
        ]
        
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem and title_elem.get_text().strip():
                title = self.clean_text(title_elem.get_text())
                if len(title) > 3:  # ç¡®ä¿æ ‡é¢˜æœ‰æ„ä¹‰
                    self.logger.debug(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æå–æ ‡é¢˜: {title[:50]}...")
                    return title
        
        return ''
    
    def _extract_main_content(self, soup: BeautifulSoup, url: str) -> str:
        """æ™ºèƒ½æå–ä¸»è¦å†…å®¹"""
        # å¯Œé€”å¸®åŠ©ä¸­å¿ƒç‰¹å®šçš„å†…å®¹é€‰æ‹©å™¨
        futu_selectors = [
            '.right-topic .futu-richTextContent',
            '.topic-preview .futu-richTextContent',
            '.futu-richTextContent',
            '.right-topic',
            '.topic-preview'
        ]
        
        # é€šç”¨å†…å®¹é€‰æ‹©å™¨
        general_selectors = [
            'div.content-box',
            '.content-box',
            '.article-content',
            '.main-content',
            '.content-body',
            '.help-content',
            'main .content',
            'article',
            'main'
        ]
        
        all_selectors = futu_selectors + general_selectors
        
        for selector in all_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # è¿›ä¸€æ­¥æ¸…ç†å†…å®¹
                self._clean_content_element(content_elem)
                
                content_text = content_elem.get_text().strip()
                if len(content_text) > self.settings.min_content_length:
                    self.logger.debug(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æå–å†…å®¹ ({len(content_text)} å­—ç¬¦)")
                    return content_text
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„å†…å®¹ï¼Œå°è¯•æå–bodyä¸­çš„æ–‡æœ¬
        body = soup.find('body')
        if body:
            self._clean_content_element(body)
            content_text = body.get_text().strip()
            if len(content_text) > self.settings.min_content_length:
                self.logger.debug(f"ä½¿ç”¨bodyæ ‡ç­¾æå–å†…å®¹ ({len(content_text)} å­—ç¬¦)")
                return content_text
        
        return ''
    
    def _clean_content_element(self, element) -> None:
        """æ¸…ç†å†…å®¹å…ƒç´ """
        # ç§»é™¤ä¸éœ€è¦çš„å­å…ƒç´ 
        unwanted_selectors = [
            'script', 'style', 'nav', 'footer', 'header',
            '.advertisement', '.ads', '.sidebar', '.navigation',
            '.feedback-box', '.helpful-box', '.social-share',
            'button', '.btn', 'input', 'form'
        ]
        
        for selector in unwanted_selectors:
            for unwanted in element.select(selector):
                unwanted.decompose()
    
    def _extract_meta_description(self, soup: BeautifulSoup) -> str:
        """æå–é¡µé¢æè¿°"""
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'):
            return self.clean_text(meta_desc['content'])
        return ''
    
    def _extract_category(self, soup: BeautifulSoup) -> str:
        """æå–åˆ†ç±»ä¿¡æ¯"""
        breadcrumb_selectors = [
            '.breadcrumb',
            '.breadcrumbs', 
            '.nav-breadcrumb',
            '.category-path'
        ]
        
        for selector in breadcrumb_selectors:
            breadcrumb = soup.select_one(selector)
            if breadcrumb:
                return self.clean_text(breadcrumb.get_text())
        
        return ''
    
    def _extract_tags(self, soup: BeautifulSoup) -> List[str]:
        """æå–æ ‡ç­¾"""
        tag_selectors = [
            '.tags a',
            '.tag',
            '.article-tags a',
            '.keywords'
        ]
        
        for selector in tag_selectors:
            tags = soup.select(selector)
            if tags:
                tag_list = [self.clean_text(tag.get_text()) for tag in tags]
                return [tag for tag in tag_list if tag]  # è¿‡æ»¤ç©ºæ ‡ç­¾
        
        return []
    
    def clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬å†…å®¹"""
        if not text:
            return ''
            
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text)
        # ç§»é™¤é¦–å°¾ç©ºç™½
        text = text.strip()
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', text)
        
        return text
    
    def detect_language_from_url(self, url: str) -> str:
        """æ ¹æ®URLè·¯å¾„æ£€æµ‹è¯­è¨€"""
        if '/en/' in url:
            return 'en'
        elif '/hant/' in url:
            return 'zh-hk'
        else:
            return 'zh-cn'
    
    def extract_all_links(self, soup: BeautifulSoup, base_url: str) -> Dict[str, List[str]]:
        """æå–é¡µé¢ä¸­çš„æ‰€æœ‰ç›¸å…³é“¾æ¥"""
        links = {
            'categories': [],
            'articles': [],
            'subcategories': []
        }
        
        base_domain = 'https://support.futunn.com'
        
        for link in soup.select('a[href]'):
            href = link.get('href')
            if not href:
                continue
                
            full_url = urljoin(base_url, href)
            
            # åªå¤„ç†å¯Œé€”å¸®åŠ©ä¸­å¿ƒçš„é“¾æ¥
            if not full_url.startswith(base_domain):
                continue
                
            # åˆ†ç±»é“¾æ¥
            if '/categories/' in full_url:
                links['categories'].append(full_url)
                self.stats['categories_found'] += 1
            
            # æ–‡ç« é“¾æ¥
            elif any(pattern in full_url for pattern in ['/articles/', '/help/', '/learn/']):
                links['articles'].append(full_url)
                self.stats['articles_found'] += 1
            
            # å­åˆ†ç±»é“¾æ¥
            elif '/subcategories/' in full_url:
                links['subcategories'].append(full_url)
        
        # å»é‡
        for key in links:
            links[key] = list(set(links[key]))
            
        return links
    
    def crawl_single_page(self, url: str) -> Optional[Dict]:
        """çˆ¬å–å•ä¸ªé¡µé¢ï¼Œé›†æˆè´¨é‡æ£€æµ‹å’Œå»é‡"""
        if url in self.visited_urls:
            self.logger.debug(f"URLå·²è®¿é—®ï¼Œè·³è¿‡: {url}")
            return None
            
        self.visited_urls.add(url)
        self.stats['total_processed'] += 1
        
        soup = self.get_page_content(url)
        if not soup:
            return None
            
        content = self.extract_content_from_page(soup, url)
        
        # ä½¿ç”¨è´¨é‡æ£€æµ‹å™¨éªŒè¯å†…å®¹
        quality = self.quality_checker.check_content_quality(content)
        
        if not quality.is_valid:
            self.logger.warning(f"å†…å®¹è´¨é‡ä¸è¶³ (è¯„åˆ†: {quality.score:.1f}): {url}")
            self.logger.debug(f"è´¨é‡é—®é¢˜: {', '.join(quality.issues)}")
            self.stats['quality_failed'] += 1
            return None
        
        # æ£€æŸ¥å†…å®¹å»é‡
        content_hash = content.get('content_hash', '')
        if self._is_duplicate_content(content_hash):
            self.logger.warning(f"å‘ç°é‡å¤å†…å®¹ï¼Œè·³è¿‡: {url}")
            return None
            
        self.stats['quality_passed'] += 1
        self.logger.info(f"æˆåŠŸçˆ¬å– (è´¨é‡è¯„åˆ†: {quality.score:.1f}): {content['title'][:50]}...")
        return content
    
    def _is_duplicate_content(self, content_hash: str) -> bool:
        """æ£€æŸ¥å†…å®¹æ˜¯å¦é‡å¤"""
        if not content_hash:
            return False
            
        # æ£€æŸ¥æ‰€æœ‰è¯­è¨€ç‰ˆæœ¬ä¸­æ˜¯å¦å·²å­˜åœ¨ç›¸åŒå†…å®¹
        for lang_docs in self.docs_data.values():
            for doc in lang_docs.values():
                if doc.get('content_hash') == content_hash:
                    return True
        return False
    
    def deep_crawl_category(self, category_url: str, max_depth: int = 3, max_articles: int = 50) -> List[Dict]:
        """æ·±åº¦çˆ¬å–åˆ†ç±»é¡µé¢åŠå…¶æ‰€æœ‰å­é¡µé¢ï¼Œä¼˜åŒ–å¹¶å‘æ§åˆ¶å’Œé”™è¯¯å¤„ç†"""
        self.logger.info(f"å¼€å§‹æ·±åº¦çˆ¬å–åˆ†ç±»: {category_url} (æœ€å¤§æ·±åº¦: {max_depth}, æœ€å¤§æ–‡ç« æ•°: {max_articles})")
        
        all_content = []
        urls_to_process = [category_url]
        processed_count = 0
        batch_size = min(self.settings.max_workers * 2, 20)  # é™åˆ¶æ‰¹æ¬¡å¤§å°
        
        for depth in range(max_depth):
            if not urls_to_process or processed_count >= max_articles:
                break
                
            self.logger.info(f"å¤„ç†æ·±åº¦ {depth + 1}, å¾…å¤„ç†URLæ•°: {len(urls_to_process)}")
            
            current_urls = urls_to_process[:]
            urls_to_process = []
            
            # åˆ†æ‰¹å¤„ç†URLï¼Œé¿å…è¿‡è½½
            for i in range(0, len(current_urls), batch_size):
                if processed_count >= max_articles:
                    break
                    
                batch_urls = current_urls[i:i + batch_size]
                self.logger.debug(f"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(current_urls)-1)//batch_size + 1} ({len(batch_urls)} ä¸ªURL)")
                
                # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
                with ThreadPoolExecutor(max_workers=self.settings.max_workers) as executor:
                    future_to_url = {executor.submit(self.crawl_single_page, url): url for url in batch_urls}
                    
                    for future in as_completed(future_to_url, timeout=300):  # 5åˆ†é’Ÿè¶…æ—¶
                        if processed_count >= max_articles:
                            break
                            
                        url = future_to_url[future]
                        try:
                            content = future.result(timeout=60)  # å•ä¸ªä»»åŠ¡1åˆ†é’Ÿè¶…æ—¶
                            if content:
                                all_content.append(content)
                                processed_count += 1
                                self.logger.debug(f"æˆåŠŸçˆ¬å–å†…å®¹: {content['title'][:30]}...")
                                
                                # å¦‚æœæ˜¯åˆ†ç±»é¡µé¢ï¼Œæå–æ›´å¤šé“¾æ¥
                                if depth < max_depth - 1:
                                    soup = self.get_page_content(url)
                                    if soup:
                                        links = self.extract_all_links(soup, url)
                                        # æ·»åŠ æ–°å‘ç°çš„é“¾æ¥åˆ°ä¸‹ä¸€è½®å¤„ç†
                                        for link_list in links.values():
                                            for link in link_list:
                                                if link not in self.visited_urls:
                                                    urls_to_process.append(link)
                            
                            # åŠ¨æ€å»¶è¿Ÿæ§åˆ¶
                            delay = random.uniform(*self.settings.delay_range)
                            if processed_count % 10 == 0:  # æ¯10ä¸ªå†…å®¹ç¨ä½œä¼‘æ¯
                                delay *= 1.5
                            time.sleep(delay)
                                                
                        except TimeoutError:
                            self.logger.warning(f"å¤„ç†URLè¶…æ—¶: {url}")
                            self.stats['timeout_errors'] = self.stats.get('timeout_errors', 0) + 1
                        except Exception as e:
                            self.logger.error(f"å¤„ç†URLæ—¶å‡ºé”™ {url}: {e}")
                            self.stats['crawl_errors'] = self.stats.get('crawl_errors', 0) + 1
                
                # æ‰¹æ¬¡é—´ä¼‘æ¯
                if i + batch_size < len(current_urls) and processed_count < max_articles:
                    batch_delay = random.uniform(2, 5)
                    self.logger.debug(f"æ‰¹æ¬¡é—´ä¼‘æ¯ {batch_delay:.1f} ç§’")
                    time.sleep(batch_delay)
            
            # å»é‡ä¸‹ä¸€è½®è¦å¤„ç†çš„URL
            urls_to_process = list(dict.fromkeys(urls_to_process))  # ä¿æŒé¡ºåºçš„å»é‡
            
        self.logger.info(f"åˆ†ç±» {category_url} æ·±åº¦çˆ¬å–å®Œæˆï¼Œå…±è·å¾— {len(all_content)} ç¯‡æ–‡æ¡£")
        return all_content
    
    def save_documents(self, output_dir: str = 'docs_deep'):
        """ä¿å­˜æ–‡æ¡£ï¼Œå¢å¼ºæ–‡ä»¶ä¿å­˜åŠŸèƒ½å’Œé”™è¯¯å¤„ç†"""
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        for lang, docs in self.docs_data.items():
            if not docs:
                continue
                
            lang_dir = os.path.join(output_dir, lang)
            if not os.path.exists(lang_dir):
                os.makedirs(lang_dir)
                
            # ä¿å­˜JSONæ ¼å¼
            json_file = os.path.join(lang_dir, f'futu_docs_deep_{timestamp}.json')
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(docs, f, ensure_ascii=False, indent=2)
                
            # ä¿å­˜Markdownæ ¼å¼
            md_file = os.path.join(lang_dir, f'futu_docs_deep_{timestamp}.md')
            with open(md_file, 'w', encoding='utf-8') as f:
                lang_names = {
                    'zh-cn': 'ç®€ä½“ä¸­æ–‡',
                    'zh-hk': 'ç¹ä½“ä¸­æ–‡ï¼ˆé¦™æ¸¯ï¼‰',
                    'en': 'English'
                }
                
                f.write(f"# å¯Œé€”ç‰›ç‰›å¸®åŠ©ä¸­å¿ƒæ·±åº¦çˆ¬å–æ–‡æ¡£ - {lang_names.get(lang, lang.upper())}\n\n")
                f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"æ–‡æ¡£æ•°é‡: {len(docs)}\n\n")
                f.write("---\n\n")
                
                for url, content in docs.items():
                    f.write(f"## {content['title']}\n\n")
                    f.write(f"**æ¥æº**: {content['url']}\n\n")
                    f.write(f"**çˆ¬å–æ—¶é—´**: {content.get('crawl_time', 'N/A')}\n\n")
                    
                    if content.get('category'):
                        f.write(f"**åˆ†ç±»**: {content['category']}\n\n")
                        
                    if content.get('tags'):
                        f.write(f"**æ ‡ç­¾**: {', '.join(content['tags'])}\n\n")
                        
                    f.write(f"{content['content']}\n\n")
                    f.write("---\n\n")
                    
            # ä¿å­˜å•ä¸ªæ–‡æ¡£æ–‡ä»¶
            for url, content in docs.items():
                self.save_to_file(content, lang)
                    
            self.logger.info(f"å·²ä¿å­˜ {lang} æ–‡æ¡£: {len(docs)} ç¯‡")
            self.logger.info(f"JSONæ–‡ä»¶: {json_file}")
            self.logger.info(f"Markdownæ–‡ä»¶: {md_file}")
    
    def save_to_file(self, content: Dict, language: str = 'zh') -> bool:
        """ä¿å­˜å†…å®¹åˆ°æ–‡ä»¶ï¼Œæ”¯æŒå¤šç§æ ¼å¼å’Œå¢å¼ºé”™è¯¯å¤„ç†"""
        try:
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = Path(self.settings.output_dir) / f"docs_{language}"
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
            title = content.get('title', 'æœªçŸ¥æ ‡é¢˜')
            safe_title = self._generate_safe_filename(title)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼ˆé¿å…é‡å¤ä¿å­˜ï¼‰
            base_filename = f"{safe_title[:50]}"
            filepath = self._get_unique_filepath(output_dir, base_filename, '.txt')
            
            # å‡†å¤‡æ–‡ä»¶å†…å®¹
            file_content = self._format_content_for_file(content)
            
            # å†™å…¥æ–‡ä»¶
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(file_content)
                
            # éªŒè¯æ–‡ä»¶å†™å…¥
            if filepath.exists() and filepath.stat().st_size > 0:
                self.logger.info(f"å†…å®¹å·²ä¿å­˜åˆ°: {filepath} ({filepath.stat().st_size} å­—èŠ‚)")
                self.stats['files_saved'] = self.stats.get('files_saved', 0) + 1
                return True
            else:
                self.logger.error(f"æ–‡ä»¶ä¿å­˜å¤±è´¥æˆ–æ–‡ä»¶ä¸ºç©º: {filepath}")
                return False
                
        except PermissionError:
            self.logger.error(f"æ²¡æœ‰æƒé™å†™å…¥æ–‡ä»¶: {filepath}")
            return False
        except OSError as e:
            self.logger.error(f"æ–‡ä»¶ç³»ç»Ÿé”™è¯¯: {e}")
            return False
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return False
    
    def _generate_safe_filename(self, title: str) -> str:
        """ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å"""
        # ç§»é™¤æˆ–æ›¿æ¢éæ³•å­—ç¬¦
        safe_title = re.sub(r'[<>:"/\\|?*]', '_', title)
        safe_title = re.sub(r'[\s]+', '_', safe_title)  # å¤šä¸ªç©ºæ ¼æ›¿æ¢ä¸ºå•ä¸ªä¸‹åˆ’çº¿
        safe_title = safe_title.strip('._')  # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„ç‚¹å’Œä¸‹åˆ’çº¿
        
        # ç¡®ä¿æ–‡ä»¶åä¸ä¸ºç©º
        if not safe_title:
            safe_title = f"document_{int(time.time())}"
            
        return safe_title
    
    def _get_unique_filepath(self, directory: Path, base_name: str, extension: str) -> Path:
        """è·å–å”¯ä¸€çš„æ–‡ä»¶è·¯å¾„ï¼Œé¿å…è¦†ç›–"""
        filepath = directory / f"{base_name}{extension}"
        counter = 1
        
        while filepath.exists():
            filepath = directory / f"{base_name}_{counter}{extension}"
            counter += 1
            
        return filepath
    
    def _format_content_for_file(self, content: Dict) -> str:
        """æ ¼å¼åŒ–å†…å®¹ç”¨äºæ–‡ä»¶ä¿å­˜"""
        lines = []
        
        # åŸºæœ¬ä¿¡æ¯
        lines.append(f"æ ‡é¢˜: {content.get('title', 'æœªçŸ¥æ ‡é¢˜')}")
        lines.append(f"URL: {content.get('url', '')}")
        lines.append(f"åˆ†ç±»: {content.get('category', 'æœªçŸ¥')}")
        
        # æ ‡ç­¾
        tags = content.get('tags', [])
        if tags:
            lines.append(f"æ ‡ç­¾: {', '.join(tags)}")
        
        # å…ƒæ•°æ®
        if content.get('meta_description'):
            lines.append(f"æè¿°: {content['meta_description']}")
        
        lines.append(f"å­—æ•°: {content.get('word_count', 0)}")
        lines.append(f"å†…å®¹å“ˆå¸Œ: {content.get('content_hash', '')}")
        lines.append(f"çˆ¬å–æ—¶é—´: {content.get('crawl_time', '')}")
        
        # åˆ†éš”çº¿
        lines.append("\n" + "="*80 + "\n")
        
        # ä¸»è¦å†…å®¹
        main_content = content.get('content', '')
        if isinstance(main_content, list):
            main_content = '\n\n'.join(main_content)
        
        lines.append(main_content)
        
        return '\n'.join(lines)
    
    def run_deep_crawl(self, urls: List[str], max_depth: int = 3, max_articles_per_category: int = 50):
        """è¿è¡Œæ·±åº¦çˆ¬è™«ï¼Œå¢åŠ ç»Ÿè®¡ä¿¡æ¯å’Œæ€§èƒ½ç›‘æ§"""
        self.stats['start_time'] = time.time()
        self.logger.info("å¼€å§‹è¿è¡Œæ·±åº¦å¯Œé€”ç‰›ç‰›å¸®åŠ©ä¸­å¿ƒæ–‡æ¡£çˆ¬è™«...")
        self.logger.info(f"ç›®æ ‡URLæ•°é‡: {len(urls)}")
        self.logger.info(f"æœ€å¤§æ·±åº¦: {max_depth}")
        self.logger.info(f"æ¯ä¸ªåˆ†ç±»æœ€å¤§æ–‡ç« æ•°: {max_articles_per_category}")
        self.logger.info(f"å¹¶å‘è®¾ç½®: æœ€å¤§å·¥ä½œçº¿ç¨‹={self.settings.max_workers}, å»¶è¿ŸèŒƒå›´={self.settings.delay_range}")
        
        successful_categories = 0
        failed_categories = 0
        
        for i, url in enumerate(urls, 1):
            category_start_time = time.time()
            self.logger.info(f"\n{'='*60}")
            self.logger.info(f"å¤„ç†ç¬¬ {i}/{len(urls)} ä¸ªåˆ†ç±»: {url}")
            
            try:
                # æ£€æµ‹è¯­è¨€
                lang = self.detect_language_from_url(url)
                self.logger.info(f"æ£€æµ‹åˆ°è¯­è¨€: {lang}")
                
                # æ·±åº¦çˆ¬å–åˆ†ç±»å’Œæ–‡ç« 
                articles = self.deep_crawl_category(url, max_depth, max_articles_per_category)
                
                if articles:
                    successful_categories += 1
                    category_time = time.time() - category_start_time
                    self.logger.info(f"åˆ†ç±» {i} å®Œæˆï¼Œè€—æ—¶ {category_time:.1f}ç§’ï¼Œè·å¾— {len(articles)} ç¯‡æ–‡ç« ")
                    
                    # ä¿å­˜åˆ°å¯¹åº”è¯­è¨€çš„æ•°æ®ä¸­
                    for article in articles:
                        self.docs_data[lang][article['url']] = article
                        
                    # å®æ—¶ç»Ÿè®¡æ›´æ–°
                    self.stats['categories_processed'] = successful_categories
                    self.stats['total_articles'] = sum(len(docs) for docs in self.docs_data.values())
                    
                else:
                    failed_categories += 1
                    self.logger.warning(f"åˆ†ç±» {i} æœªè·å–åˆ°ä»»ä½•æ–‡ç« ")
                    
            except KeyboardInterrupt:
                self.logger.info("ç”¨æˆ·ä¸­æ–­çˆ¬å–è¿‡ç¨‹")
                break
            except Exception as e:
                failed_categories += 1
                self.logger.error(f"æ·±åº¦çˆ¬å–å¤±è´¥ {url}: {e}")
                self.stats['category_errors'] = self.stats.get('category_errors', 0) + 1
                
            # åˆ†ç±»é—´ä¼‘æ¯
            if i < len(urls):
                inter_category_delay = random.uniform(3, 8)
                self.logger.info(f"åˆ†ç±»é—´ä¼‘æ¯ {inter_category_delay:.1f} ç§’")
                time.sleep(inter_category_delay)
        
        # è®°å½•ç»“æŸæ—¶é—´
        self.stats['end_time'] = time.time()
        total_time = self.stats['end_time'] - self.stats['start_time']
        
        # ä¿å­˜æ–‡æ¡£
        self.logger.info(f"\n{'='*60}")
        self.logger.info("å¼€å§‹ä¿å­˜æ–‡æ¡£...")
        try:
            self.save_documents()
            self.logger.info("æ–‡æ¡£ä¿å­˜å®Œæˆ")
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ–‡æ¡£æ—¶å‡ºé”™: {e}")
        
        # è¾“å‡ºè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
        self._print_final_statistics(successful_categories, failed_categories, total_time)
        
        return self.docs_data, self.stats
    
    def _print_final_statistics(self, successful_categories: int, failed_categories: int, total_time: float):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        total_docs = sum(len(docs) for docs in self.docs_data.values())
        
        self.logger.info(f"\n{'='*60}")
        self.logger.info("ğŸ‰ æ·±åº¦çˆ¬å–å®Œæˆï¼")
        self.logger.info(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        self.logger.info(f"  â±ï¸  æ€»è€—æ—¶: {total_time:.1f} ç§’ ({total_time/60:.1f} åˆ†é’Ÿ)")
        self.logger.info(f"  ğŸ“ æˆåŠŸåˆ†ç±»: {successful_categories}")
        self.logger.info(f"  âŒ å¤±è´¥åˆ†ç±»: {failed_categories}")
        self.logger.info(f"  ğŸ“„ æ€»æ–‡æ¡£æ•°: {total_docs} ç¯‡")
        
        if total_docs > 0:
            avg_time_per_doc = total_time / total_docs
            self.logger.info(f"  âš¡ å¹³å‡æ¯ç¯‡: {avg_time_per_doc:.1f} ç§’")
        
        self.logger.info(f"\nğŸŒ è¯­è¨€åˆ†å¸ƒ:")
        lang_names = {
            'zh-cn': 'ç®€ä½“ä¸­æ–‡',
            'zh-hk': 'ç¹ä½“ä¸­æ–‡ï¼ˆé¦™æ¸¯ï¼‰',
            'en': 'English'
        }
        for lang, docs in self.docs_data.items():
            percentage = (len(docs) / total_docs * 100) if total_docs > 0 else 0
            lang_display = lang_names.get(lang, lang)
            self.logger.info(f"  {lang_display}: {len(docs)} ç¯‡ ({percentage:.1f}%)")
        
        self.logger.info(f"\nğŸ”§ æ€§èƒ½ç»Ÿè®¡:")
        self.logger.info(f"  ğŸ“Š æ€»å¤„ç†æ•°: {self.stats.get('total_processed', 0)}")
        self.logger.info(f"  âœ… æˆåŠŸçˆ¬å–: {self.stats.get('successful_crawls', 0)}")
        self.logger.info(f"  âŒ å¤±è´¥çˆ¬å–: {self.stats.get('failed_crawls', 0)}")
        self.logger.info(f"  âœ… è´¨é‡é€šè¿‡: {self.stats.get('quality_passed', 0)}")
        self.logger.info(f"  âŒ è´¨é‡å¤±è´¥: {self.stats.get('quality_failed', 0)}")
        self.logger.info(f"  ğŸ’¾ ç¼“å­˜å‘½ä¸­: {self.stats.get('cache_hits', 0)}")
        self.logger.info(f"  ğŸ’¾ æ–‡ä»¶ä¿å­˜: {self.stats.get('files_saved', 0)}")
        self.logger.info(f"  ğŸ” å‘ç°åˆ†ç±»: {self.stats.get('categories_found', 0)}")
        self.logger.info(f"  ğŸ“° å‘ç°æ–‡ç« : {self.stats.get('articles_found', 0)}")
        
        if self.stats.get('crawl_errors', 0) > 0:
            self.logger.info(f"  âš ï¸  çˆ¬å–é”™è¯¯: {self.stats.get('crawl_errors', 0)}")
        if self.stats.get('timeout_errors', 0) > 0:
            self.logger.info(f"  â° è¶…æ—¶é”™è¯¯: {self.stats.get('timeout_errors', 0)}")
        if self.stats.get('category_errors', 0) > 0:
            self.logger.info(f"  ğŸ“ åˆ†ç±»é”™è¯¯: {self.stats.get('category_errors', 0)}")
        
        self.logger.info(f"\nğŸ“ è¾“å‡ºç›®å½•: docs_deep/")
        self.logger.info(f"{'='*60}")