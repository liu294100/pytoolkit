#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•é“¾æ¥æå–åŠŸèƒ½
éªŒè¯çˆ¬è™«æ˜¯å¦èƒ½æ­£ç¡®æå–å­é¡¹é“¾æ¥å’Œç›¸å…³é“¾æ¥
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.core.deep_crawler import DeepFutuDocCrawler, CrawlerSettings
import logging

def test_link_extraction():
    """æµ‹è¯•é“¾æ¥æå–åŠŸèƒ½"""
    print("ğŸ” æµ‹è¯•é“¾æ¥æå–åŠŸèƒ½...")
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # åˆ›å»ºçˆ¬è™«è®¾ç½®
    settings = CrawlerSettings(
        max_workers=2,
        delay_range=(1, 3),
        timeout=30,
        retries=2,
        output_dir='test_output'
    )
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = DeepFutuDocCrawler(settings)
    
    # æµ‹è¯•URL - å¯Œé€”å¸®åŠ©ä¸­å¿ƒçš„è‚¡ç¥¨å…¥é—¨åˆ†ç±»é¡µé¢
    test_urls = [
        'https://support.futunn.com/zh-cn/categories/360000010781',  # è‚¡ç¥¨å…¥é—¨
        'https://support.futunn.com/zh-cn/topic37',  # å…·ä½“æ–‡ç« 
    ]
    
    print(f"\nğŸ“‹ æµ‹è¯•URLåˆ—è¡¨:")
    for i, url in enumerate(test_urls, 1):
        print(f"  {i}. {url}")
    
    print(f"\nğŸš€ å¼€å§‹æµ‹è¯•...")
    
    for i, url in enumerate(test_urls, 1):
        print(f"\n{'='*60}")
        print(f"æµ‹è¯• {i}/{len(test_urls)}: {url}")
        
        try:
            # è·å–é¡µé¢å†…å®¹
            soup = crawler.get_page_content(url)
            if not soup:
                print(f"âŒ æ— æ³•è·å–é¡µé¢å†…å®¹: {url}")
                continue
            
            # æå–å†…å®¹
            content = crawler.extract_content_from_page(soup, url)
            
            print(f"âœ… æˆåŠŸæå–å†…å®¹:")
            print(f"   æ ‡é¢˜: {content['title'][:50]}...")
            print(f"   å†…å®¹é•¿åº¦: {len(content['content'])} å­—ç¬¦")
            print(f"   è¯­è¨€: {content['language']}")
            
            # æ£€æŸ¥å­é¡¹é“¾æ¥
            sub_items = content.get('sub_items', [])
            print(f"   å­é¡¹é“¾æ¥: {len(sub_items)} ä¸ª")
            if sub_items:
                print(f"   å‰3ä¸ªå­é¡¹:")
                for j, item in enumerate(sub_items[:3], 1):
                    print(f"     {j}. {item['title'][:30]}... -> {item['url']}")
            
            # æ£€æŸ¥ç›¸å…³é“¾æ¥
            related_links = content.get('related_links', [])
            print(f"   ç›¸å…³é“¾æ¥: {len(related_links)} ä¸ª")
            if related_links:
                print(f"   å‰3ä¸ªç›¸å…³é“¾æ¥:")
                for j, link in enumerate(related_links[:3], 1):
                    print(f"     {j}. {link['title'][:30]}... -> {link['url']}")
            
            # æµ‹è¯•extract_all_linksæ–¹æ³•
            all_links = crawler.extract_all_links(soup, url)
            print(f"   æ‰€æœ‰é“¾æ¥ç»Ÿè®¡:")
            print(f"     åˆ†ç±»é“¾æ¥: {len(all_links['categories'])} ä¸ª")
            print(f"     æ–‡ç« é“¾æ¥: {len(all_links['articles'])} ä¸ª")
            print(f"     å­åˆ†ç±»é“¾æ¥: {len(all_links['subcategories'])} ä¸ª")
            
            if all_links['articles']:
                print(f"   å‰3ä¸ªæ–‡ç« é“¾æ¥:")
                for j, article_url in enumerate(all_links['articles'][:3], 1):
                    print(f"     {j}. {article_url}")
                    
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    print(f"\n{'='*60}")
    print("ğŸ‰ é“¾æ¥æå–åŠŸèƒ½æµ‹è¯•å®Œæˆï¼")
    print(f"\nğŸ“Š çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯:")
    for key, value in crawler.stats.items():
        print(f"   {key}: {value}")

if __name__ == '__main__':
    test_link_extraction()