# ğŸš€ Crawl4AI å¯Œé€”æ–‡æ¡£çˆ¬è™«

åŸºäº [Crawl4AI](https://docs.crawl4ai.com/) æ¡†æ¶çš„é«˜æ€§èƒ½å¯Œé€”ç‰›ç‰›å¸®åŠ©æ–‡æ¡£çˆ¬è™«å·¥å…·ã€‚

## âœ¨ ä¸»è¦ç‰¹æ€§

### ğŸ”¥ Crawl4AI ä¼˜åŠ¿
- **ğŸ¤– AI å‹å¥½**: ä¸“ä¸º LLM å’Œ AI åº”ç”¨è®¾è®¡çš„å†…å®¹æå–
- **âš¡ é«˜æ€§èƒ½**: å¼‚æ­¥å¹¶å‘çˆ¬å–ï¼Œé€Ÿåº¦æå‡ 10x+
- **ğŸ§  æ™ºèƒ½æå–**: è‡ªåŠ¨è¯†åˆ«å’Œæå–ç»“æ„åŒ–å†…å®¹
- **ğŸŒ å¤šè¯­è¨€**: è‡ªåŠ¨æ£€æµ‹å’Œåˆ†ç±»ä¸­è‹±æ–‡å†…å®¹
- **ğŸ“Š ç»“æ„åŒ–è¾“å‡º**: JSON + Markdown åŒæ ¼å¼å¯¼å‡º

### ğŸ› ï¸ æŠ€æœ¯ç‰¹æ€§
- **å¼‚æ­¥çˆ¬å–**: åŸºäº `asyncio` çš„é«˜å¹¶å‘å¤„ç†
- **æ™ºèƒ½é‡è¯•**: è‡ªåŠ¨é”™è¯¯æ¢å¤å’Œé‡è¯•æœºåˆ¶
- **å†…å®¹åˆ†å—**: ä½¿ç”¨ `RegexChunking` ä¼˜åŒ–å¤§æ–‡æ¡£å¤„ç†
- **è¯­ä¹‰æå–**: `CosineStrategy` æå–æœ€ç›¸å…³å†…å®¹
- **å®æ—¶ç›‘æ§**: è¯¦ç»†çš„è¿›åº¦è·Ÿè¸ªå’Œæ€§èƒ½ç»Ÿè®¡

## ğŸ“¦ å®‰è£…ä¾èµ–

```bash
# å®‰è£… Crawl4AI å’Œç›¸å…³ä¾èµ–
pip install crawl4ai>=0.7.0 aiohttp>=3.8.0 aiofiles>=23.0.0

# å®‰è£…å…¶ä»–ä¾èµ–
pip install -r requirements.txt
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åŸºç¡€æ¼”ç¤º

```bash
# è¿è¡ŒåŸºç¡€æ¼”ç¤º
python demo_crawl4ai.py
```

### 2. è‡ªå®šä¹‰çˆ¬å–

```bash
# æŒ‡å®šè‡ªå®šä¹‰URL
python demo_crawl4ai.py --urls https://support.futunn.com/categories/2186

# é«˜å¹¶å‘æ¨¡å¼
python demo_crawl4ai.py --max-concurrent 10 --max-depth 3

# å¤šè¯­è¨€çˆ¬å–
python demo_crawl4ai.py --urls \
  https://support.futunn.com/categories/2186 \
  https://support.futunn.com/en/categories/2186
```

### 3. GUI ç•Œé¢

```bash
# å¯åŠ¨ Crawl4AI GUI
python main_crawl4ai.py

# æˆ–ä½¿ç”¨åŸç‰ˆGUI
python main.py
```

## ğŸ“‹ ä½¿ç”¨æ–¹å¼å¯¹æ¯”

| ç‰¹æ€§ | åŸç‰ˆçˆ¬è™« | Crawl4AI çˆ¬è™« |
|------|----------|---------------|
| **æ€§èƒ½** | åŒæ­¥ï¼Œè¾ƒæ…¢ | å¼‚æ­¥ï¼Œ10x+ é€Ÿåº¦æå‡ |
| **å†…å®¹è´¨é‡** | åŸºç¡€HTMLè§£æ | AIä¼˜åŒ–çš„æ™ºèƒ½æå– |
| **å¹¶å‘å¤„ç†** | çº¿ç¨‹æ±  | åŸç”Ÿå¼‚æ­¥åç¨‹ |
| **é”™è¯¯å¤„ç†** | åŸºç¡€é‡è¯• | æ™ºèƒ½æ¢å¤æœºåˆ¶ |
| **è¾“å‡ºæ ¼å¼** | JSON | JSON + Markdown |
| **è¯­è¨€æ”¯æŒ** | æ‰‹åŠ¨åˆ†ç±» | è‡ªåŠ¨æ£€æµ‹åˆ†ç±» |
| **å†…å­˜ä½¿ç”¨** | è¾ƒé«˜ | ä¼˜åŒ–çš„æµå¼å¤„ç† |

## âš™ï¸ é…ç½®å‚æ•°

### Crawl4AI è®¾ç½®

```python
from src.core.crawl4ai_crawler import Crawl4AISettings

settings = Crawl4AISettings(
    max_concurrent=5,        # æœ€å¤§å¹¶å‘æ•°
    delay_range=(1.0, 2.0),  # è¯·æ±‚å»¶è¿ŸèŒƒå›´(ç§’)
    timeout=30,              # è¯·æ±‚è¶…æ—¶æ—¶é—´
    output_dir='output',     # è¾“å‡ºç›®å½•
    headless=True,           # æ— å¤´æµè§ˆå™¨æ¨¡å¼
    enable_js=True,          # å¯ç”¨JavaScript
    screenshot=False,        # æ˜¯å¦æˆªå›¾
    wait_for_images=False    # æ˜¯å¦ç­‰å¾…å›¾ç‰‡åŠ è½½
)
```

### çˆ¬å–å‚æ•°

```python
results, info = await run_crawl4ai_crawler(
    urls=['https://support.futunn.com/categories/2186'],
    max_depth=2,             # æœ€å¤§çˆ¬å–æ·±åº¦
    settings=settings
)
```

## ğŸ“Š è¾“å‡ºæ ¼å¼

### JSON æ ¼å¼
```json
{
  "url": "https://support.futunn.com/articles/123",
  "title": "å¦‚ä½•å¼€æˆ·",
  "content": "è¯¦ç»†çš„å¼€æˆ·æµç¨‹...",
  "language": "zh",
  "links": ["https://..."],
  "crawl_time": "2024-01-01T12:00:00",
  "success": true
}
```

### Markdown æ ¼å¼
```markdown
# å¦‚ä½•å¼€æˆ·

**URL**: https://support.futunn.com/articles/123  
**è¯­è¨€**: ä¸­æ–‡  
**çˆ¬å–æ—¶é—´**: 2024-01-01 12:00:00

## å†…å®¹

è¯¦ç»†çš„å¼€æˆ·æµç¨‹...

## ç›¸å…³é“¾æ¥
- [é“¾æ¥1](https://...)
- [é“¾æ¥2](https://...)
```

## ğŸ”§ API ä½¿ç”¨

### åŸºç¡€ç”¨æ³•

```python
import asyncio
from src.core.crawl4ai_crawler import run_crawl4ai_crawler, Crawl4AISettings

async def main():
    # é…ç½®è®¾ç½®
    settings = Crawl4AISettings(
        max_concurrent=3,
        output_dir='my_output'
    )
    
    # è¿è¡Œçˆ¬å–
    results, info = await run_crawl4ai_crawler(
        urls=['https://support.futunn.com/categories/2186'],
        max_depth=2,
        settings=settings
    )
    
    # å¤„ç†ç»“æœ
    for result in results:
        if result.success:
            print(f"æˆåŠŸçˆ¬å–: {result.title}")
            print(f"å†…å®¹é•¿åº¦: {len(result.content)}")
        else:
            print(f"çˆ¬å–å¤±è´¥: {result.url} - {result.error}")
    
    # æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
    stats = info['stats']
    print(f"æ€»å…±å¤„ç†: {stats['total_processed']} é¡µé¢")
    print(f"æˆåŠŸçˆ¬å–: {stats['successful_crawls']} é¡µé¢")

# è¿è¡Œ
asyncio.run(main())
```

### é«˜çº§ç”¨æ³•

```python
from src.core.crawl4ai_crawler import Crawl4AIFutuCrawler, Crawl4AISettings

async def advanced_crawl():
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    settings = Crawl4AISettings(
        max_concurrent=10,
        delay_range=(0.5, 1.0),
        enable_js=True,
        screenshot=True  # å¯ç”¨æˆªå›¾
    )
    
    crawler = Crawl4AIFutuCrawler(settings)
    
    # å•é¡µé¢çˆ¬å–
    result = await crawler.crawl_single_page(
        "https://support.futunn.com/articles/123"
    )
    
    if result.success:
        print(f"æ ‡é¢˜: {result.title}")
        print(f"è¯­è¨€: {result.language}")
        print(f"é“¾æ¥æ•°: {len(result.links)}")
    
    # æ‰¹é‡çˆ¬å–
    urls = [
        "https://support.futunn.com/categories/2186",
        "https://support.futunn.com/categories/2187"
    ]
    
    all_results = await crawler.crawl_batch_urls(
        urls, max_depth=3
    )
    
    # ä¿å­˜ç»“æœ
    saved_files = await crawler.save_results(
        all_results, output_dir="advanced_output"
    )
    
    print(f"ä¿å­˜çš„æ–‡ä»¶: {saved_files}")

asyncio.run(advanced_crawl())
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### 1. å¹¶å‘è°ƒä¼˜
```python
# é«˜æ€§èƒ½è®¾ç½®ï¼ˆé€‚åˆæœåŠ¡å™¨ç¯å¢ƒï¼‰
settings = Crawl4AISettings(
    max_concurrent=20,       # é«˜å¹¶å‘
    delay_range=(0.1, 0.5), # çŸ­å»¶è¿Ÿ
    timeout=60,              # é•¿è¶…æ—¶
    headless=True,           # æ— å¤´æ¨¡å¼
    wait_for_images=False    # è·³è¿‡å›¾ç‰‡
)

# ç¨³å®šè®¾ç½®ï¼ˆé€‚åˆä¸ªäººç”µè„‘ï¼‰
settings = Crawl4AISettings(
    max_concurrent=5,        # ä¸­ç­‰å¹¶å‘
    delay_range=(1.0, 2.0), # é€‚ä¸­å»¶è¿Ÿ
    timeout=30,              # æ ‡å‡†è¶…æ—¶
    headless=True,
    enable_js=True
)
```

### 2. å†…å­˜ä¼˜åŒ–
```python
# å¤§è§„æ¨¡çˆ¬å–æ—¶çš„å†…å­˜ä¼˜åŒ–
settings = Crawl4AISettings(
    max_concurrent=3,        # é™ä½å¹¶å‘
    screenshot=False,        # ç¦ç”¨æˆªå›¾
    wait_for_images=False,   # è·³è¿‡å›¾ç‰‡
    # ä½¿ç”¨æµå¼å¤„ç†
)

# åˆ†æ‰¹å¤„ç†å¤§é‡URL
urls = [...]  # å¤§é‡URLåˆ—è¡¨
batch_size = 50

for i in range(0, len(urls), batch_size):
    batch_urls = urls[i:i+batch_size]
    results, _ = await run_crawl4ai_crawler(
        urls=batch_urls,
        max_depth=1,
        settings=settings
    )
    # å¤„ç†å½“å‰æ‰¹æ¬¡ç»“æœ
    process_batch_results(results)
```

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **å®‰è£…é—®é¢˜**
```bash
# å¦‚æœ crawl4ai å®‰è£…å¤±è´¥
pip install --upgrade pip
pip install crawl4ai --no-cache-dir

# æˆ–ä½¿ç”¨conda
conda install -c conda-forge crawl4ai
```

2. **æµè§ˆå™¨é—®é¢˜**
```python
# å¦‚æœæµè§ˆå™¨å¯åŠ¨å¤±è´¥ï¼Œå°è¯•ç¦ç”¨JS
settings = Crawl4AISettings(
    enable_js=False,
    headless=True
)
```

3. **å†…å­˜ä¸è¶³**
```python
# é™ä½å¹¶å‘æ•°å’Œå¯ç”¨æµå¼å¤„ç†
settings = Crawl4AISettings(
    max_concurrent=2,
    screenshot=False,
    wait_for_images=False
)
```

4. **ç½‘ç»œè¶…æ—¶**
```python
# å¢åŠ è¶…æ—¶æ—¶é—´å’Œé‡è¯•
settings = Crawl4AISettings(
    timeout=60,
    delay_range=(2.0, 5.0)
)
```

### è°ƒè¯•æ¨¡å¼

```python
import logging

# å¯ç”¨è¯¦ç»†æ—¥å¿—
logging.basicConfig(level=logging.DEBUG)

# æˆ–åœ¨ä»£ç ä¸­è®¾ç½®
crawler = Crawl4AIFutuCrawler(settings)
crawler.logger.setLevel(logging.DEBUG)
```

## ğŸ“ æ›´æ–°æ—¥å¿—

### v2.0.0 (Crawl4AIç‰ˆæœ¬)
- âœ… é›†æˆ Crawl4AI æ¡†æ¶
- âœ… å¼‚æ­¥å¹¶å‘çˆ¬å–
- âœ… AI ä¼˜åŒ–çš„å†…å®¹æå–
- âœ… è‡ªåŠ¨è¯­è¨€æ£€æµ‹
- âœ… ç»“æ„åŒ–è¾“å‡ºæ ¼å¼
- âœ… å®æ—¶è¿›åº¦ç›‘æ§
- âœ… æ™ºèƒ½é”™è¯¯æ¢å¤

### v1.0.0 (åŸç‰ˆ)
- âœ… åŸºç¡€HTMLè§£æ
- âœ… å¤šçº¿ç¨‹çˆ¬å–
- âœ… GUIç•Œé¢
- âœ… JSONè¾“å‡º

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## ğŸ“„ è®¸å¯è¯

MIT License

## ğŸ”— ç›¸å…³é“¾æ¥

- [Crawl4AI å®˜æ–¹æ–‡æ¡£](https://docs.crawl4ai.com/)
- [å¯Œé€”ç‰›ç‰›å¸®åŠ©ä¸­å¿ƒ](https://support.futunn.com/)
- [é¡¹ç›®ä»“åº“](https://github.com/your-repo/fetchFutuDoc)