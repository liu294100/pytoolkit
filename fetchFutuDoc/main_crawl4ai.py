#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¯Œé€”ç‰›ç‰›å¸®åŠ©ä¸­å¿ƒ Crawl4AI ä¼˜åŒ–çˆ¬è™«ä¸»å¯åŠ¨è„šæœ¬
Futu Help Center Crawl4AI Optimized Crawler Main Entry Point

ä½¿ç”¨æ–¹æ³•:
1. GUIæ¨¡å¼: python main_crawl4ai.py
2. å‘½ä»¤è¡Œæ¨¡å¼: python main_crawl4ai.py --cli
3. å¸®åŠ©ä¿¡æ¯: python main_crawl4ai.py --help

æ–°ç‰¹æ€§:
- åŸºäº Crawl4AI çš„é«˜æ€§èƒ½å¼‚æ­¥çˆ¬å–
- AI å‹å¥½çš„å†…å®¹æå–
- æ™ºèƒ½é€‚åº”æ€§çˆ¬å–
- æ›´å¥½çš„å¹¶å‘æ§åˆ¶
- ç»“æ„åŒ–æ•°æ®è¾“å‡º
"""

import sys
import os
import argparse
import asyncio
from typing import List
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

def check_crawl4ai_installation():
    """æ£€æŸ¥ Crawl4AI æ˜¯å¦å·²å®‰è£…"""
    try:
        import crawl4ai
        return True
    except ImportError:
        print("âŒ Crawl4AI æœªå®‰è£…")
        print("è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…:")
        print("pip install crawl4ai aiohttp aiofiles")
        print("\næˆ–è€…è¿è¡Œ: pip install -r requirements.txt")
        return False

def run_gui():
    """è¿è¡ŒGUIæ¨¡å¼"""
    try:
        import tkinter as tk
        from tkinter import ttk, messagebox
        from src.gui.main_window import FutuCrawlerGUI
        
        # æ£€æŸ¥ Crawl4AI
        if not check_crawl4ai_installation():
            root = tk.Tk()
            root.withdraw()
            messagebox.showerror(
                "ä¾èµ–ç¼ºå¤±", 
                "Crawl4AI æœªå®‰è£…ï¼\n\nè¯·è¿è¡Œ: pip install crawl4ai aiohttp aiofiles"
            )
            return
        
        print("å¯åŠ¨ Crawl4AI ä¼˜åŒ–ç‰ˆ GUI ç•Œé¢...")
        root = tk.Tk()
        
        # è®¾ç½®ä¸»é¢˜
        try:
            style = ttk.Style()
            style.theme_use('clam')
        except:
            pass
        
        # åˆ›å»ºå¢å¼ºç‰ˆGUIï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        try:
            from src.gui.crawl4ai_window import Crawl4AIFutuGUI
            app = Crawl4AIFutuGUI(root)
        except ImportError:
            # å›é€€åˆ°åŸå§‹GUI
            app = FutuCrawlerGUI(root)
            print("ä½¿ç”¨åŸå§‹GUIç•Œé¢ï¼ˆCrawl4AI GUIç•Œé¢æœªæ‰¾åˆ°ï¼‰")
        
        # è®¾ç½®çª—å£å…³é—­äº‹ä»¶
        def on_closing():
            if hasattr(app, 'is_crawling') and app.is_crawling:
                if messagebox.askokcancel("é€€å‡º", "çˆ¬å–æ­£åœ¨è¿›è¡Œä¸­ï¼Œç¡®å®šè¦é€€å‡ºå—ï¼Ÿ"):
                    if hasattr(app, 'stop_crawling'):
                        app.stop_crawling()
                    root.destroy()
            else:
                root.destroy()
        
        root.protocol("WM_DELETE_WINDOW", on_closing)
        root.mainloop()
        
    except ImportError as e:
        print(f"GUIä¾èµ–ç¼ºå¤±: {e}")
        print("è¯·å®‰è£…tkinter: pip install tk")
        sys.exit(1)
    except Exception as e:
        print(f"GUIå¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)

async def run_cli_async(args):
    """å¼‚æ­¥è¿è¡Œå‘½ä»¤è¡Œæ¨¡å¼"""
    try:
        from src.core.crawl4ai_crawler import (
            Crawl4AIFutuCrawler, 
            Crawl4AISettings,
            run_crawl4ai_crawler
        )
        
        # é»˜è®¤URLåˆ—è¡¨
        default_urls = [
            "https://support.futunn.com/categories/2186",  # åŸºç¡€çŸ¥è¯†å…¥é—¨
            "https://support.futunn.com/categories/2185",  # å¸‚åœºä»‹ç»
            "https://support.futunn.com/categories/2187",  # å®¢æˆ·ç«¯åŠŸèƒ½
            "https://support.futunn.com/hant/categories/2186",  # åŸºç¡€çŸ¥è¯†å…¥é—¨(ç¹ä½“)
            "https://support.futunn.com/hant/categories/2185",  # å¸‚åœºä»‹ç»(ç¹ä½“)
            "https://support.futunn.com/hant/categories/2187",  # å®¢æˆ·ç«¯åŠŸèƒ½(ç¹ä½“)
            "https://support.futunn.com/en/categories/2186",  # Getting started
            "https://support.futunn.com/en/categories/2185",  # Market Introduction
            "https://support.futunn.com/en/categories/2187",  # App Features
        ]
        
        urls = args.urls if args.urls else default_urls
        
        print("ğŸš€ ä½¿ç”¨ Crawl4AI æ¡†æ¶è¿›è¡Œæ·±åº¦çˆ¬å–")
        print(f"ğŸ“Š ç›®æ ‡URLæ•°é‡: {len(urls)}")
        print(f"âš™ï¸  å‚æ•°: æœ€å¤§æ·±åº¦={args.max_depth}, å¹¶å‘æ•°={args.max_concurrent}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
        print(f"ğŸ¯ å¯ç”¨JS: {args.enable_js}, æ— å¤´æ¨¡å¼: {args.headless}")
        print("-" * 60)
        
        # åˆ›å»º Crawl4AI è®¾ç½®
        settings = Crawl4AISettings(
            max_concurrent=args.max_concurrent,
            delay_range=(args.delay_min, args.delay_max),
            timeout=args.timeout,
            output_dir=args.output_dir,
            headless=args.headless,
            enable_js=args.enable_js,
            screenshot=args.screenshot,
            wait_for_images=args.wait_for_images
        )
        
        # è¿è¡Œå¼‚æ­¥çˆ¬å–
        results, info = await run_crawl4ai_crawler(
            urls=urls,
            max_depth=args.max_depth,
            settings=settings
        )
        
        # æ˜¾ç¤ºç»“æœç»Ÿè®¡
        stats = info['stats']
        saved_files = info['saved_files']
        
        print("\n" + "=" * 60)
        print("ğŸ‰ çˆ¬å–å®Œæˆï¼")
        print(f"ğŸ“ˆ æ€»å¤„ç†é¡µé¢: {stats['total_processed']}")
        print(f"âœ… æˆåŠŸçˆ¬å–: {stats['successful_crawls']}")
        print(f"âŒ å¤±è´¥çˆ¬å–: {stats['failed_crawls']}")
        print(f"ğŸ”— å”¯ä¸€é¡µé¢: {stats['unique_pages']}")
        print(f"ğŸ“ æ€»å†…å®¹é•¿åº¦: {stats['total_content_length']:,} å­—ç¬¦")
        
        if stats['start_time'] and stats['end_time']:
            duration = stats['end_time'] - stats['start_time']
            print(f"â±ï¸  è€—æ—¶: {duration}")
        
        print(f"\nğŸ“ æ–‡æ¡£å·²ä¿å­˜åˆ°: {args.output_dir}")
        
        # æŒ‰è¯­è¨€æ˜¾ç¤ºä¿å­˜çš„æ–‡ä»¶
        for lang, file_info in saved_files.items():
            print(f"  ğŸ“„ {lang.upper()}: {file_info['count']} ç¯‡æ–‡æ¡£")
            print(f"    JSON: {Path(file_info['json']).name}")
            print(f"    Markdown: {Path(file_info['markdown']).name}")
        
        # æ˜¾ç¤ºæ€§èƒ½æŒ‡æ ‡
        if stats['successful_crawls'] > 0:
            avg_content_length = stats['total_content_length'] / stats['successful_crawls']
            print(f"\nğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
            print(f"  å¹³å‡å†…å®¹é•¿åº¦: {avg_content_length:.0f} å­—ç¬¦/é¡µé¢")
            if stats['start_time'] and stats['end_time']:
                duration_seconds = duration.total_seconds()
                if duration_seconds > 0:
                    pages_per_second = stats['successful_crawls'] / duration_seconds
                    print(f"  çˆ¬å–é€Ÿåº¦: {pages_per_second:.2f} é¡µé¢/ç§’")
        
    except Exception as e:
        print(f"âŒ çˆ¬å–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

def run_cli(args):
    """è¿è¡Œå‘½ä»¤è¡Œæ¨¡å¼ï¼ˆåŒæ­¥åŒ…è£…ï¼‰"""
    # æ£€æŸ¥ Crawl4AI
    if not check_crawl4ai_installation():
        sys.exit(1)
    
    # è¿è¡Œå¼‚æ­¥çˆ¬å–
    asyncio.run(run_cli_async(args))

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="å¯Œé€”ç‰›ç‰›å¸®åŠ©ä¸­å¿ƒ Crawl4AI ä¼˜åŒ–çˆ¬è™«",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ğŸš€ ä½¿ç”¨ç¤ºä¾‹:
  GUIæ¨¡å¼:        python main_crawl4ai.py
  å‘½ä»¤è¡Œæ¨¡å¼:      python main_crawl4ai.py --cli
  è‡ªå®šä¹‰å‚æ•°:      python main_crawl4ai.py --cli --max-depth 3 --max-concurrent 8
  å¯ç”¨æˆªå›¾:       python main_crawl4ai.py --cli --screenshot
  æŒ‡å®šURL:       python main_crawl4ai.py --cli --urls https://support.futunn.com/categories/2186
  
ğŸ”§ Crawl4AI ç‰¹æ€§:
  - AI å‹å¥½çš„å†…å®¹æå–
  - é«˜æ€§èƒ½å¼‚æ­¥çˆ¬å–
  - æ™ºèƒ½é€‚åº”æ€§çˆ¬å–
  - ç»“æ„åŒ–æ•°æ®è¾“å‡º
  - æ”¯æŒ JavaScript æ¸²æŸ“
        """
    )
    
    parser.add_argument('--cli', action='store_true', help='ä½¿ç”¨å‘½ä»¤è¡Œæ¨¡å¼')
    parser.add_argument('--urls', nargs='+', help='è¦çˆ¬å–çš„URLåˆ—è¡¨')
    parser.add_argument('--max-depth', type=int, default=2, help='æœ€å¤§çˆ¬å–æ·±åº¦ (é»˜è®¤: 2)')
    parser.add_argument('--max-concurrent', type=int, default=5, help='æœ€å¤§å¹¶å‘æ•° (é»˜è®¤: 5)')
    parser.add_argument('--delay-min', type=float, default=1.0, help='æœ€å°å»¶è¿Ÿç§’æ•° (é»˜è®¤: 1.0)')
    parser.add_argument('--delay-max', type=float, default=2.0, help='æœ€å¤§å»¶è¿Ÿç§’æ•° (é»˜è®¤: 2.0)')
    parser.add_argument('--timeout', type=int, default=30, help='è¯·æ±‚è¶…æ—¶æ—¶é—´ (é»˜è®¤: 30)')
    parser.add_argument('--output-dir', default='docs_crawl4ai', help='è¾“å‡ºç›®å½• (é»˜è®¤: docs_crawl4ai)')
    
    # Crawl4AI ç‰¹å®šå‚æ•°
    parser.add_argument('--headless', action='store_true', default=True, help='ä½¿ç”¨æ— å¤´æµè§ˆå™¨æ¨¡å¼ (é»˜è®¤: True)')
    parser.add_argument('--no-headless', action='store_false', dest='headless', help='ç¦ç”¨æ— å¤´æµè§ˆå™¨æ¨¡å¼')
    parser.add_argument('--enable-js', action='store_true', default=True, help='å¯ç”¨JavaScriptæ¸²æŸ“ (é»˜è®¤: True)')
    parser.add_argument('--no-js', action='store_false', dest='enable_js', help='ç¦ç”¨JavaScriptæ¸²æŸ“')
    parser.add_argument('--screenshot', action='store_true', help='ä¿å­˜é¡µé¢æˆªå›¾')
    parser.add_argument('--wait-for-images', action='store_true', help='ç­‰å¾…å›¾ç‰‡åŠ è½½å®Œæˆ')
    
    args = parser.parse_args()
    
    # éªŒè¯å‚æ•°
    if args.delay_min >= args.delay_max:
        print("âŒ é”™è¯¯: æœ€å°å»¶è¿Ÿå¿…é¡»å°äºæœ€å¤§å»¶è¿Ÿ")
        sys.exit(1)
    
    if args.max_depth < 1 or args.max_depth > 5:
        print("âŒ é”™è¯¯: æœ€å¤§æ·±åº¦å¿…é¡»åœ¨1-5ä¹‹é—´")
        sys.exit(1)
    
    if args.max_concurrent < 1 or args.max_concurrent > 20:
        print("âŒ é”™è¯¯: æœ€å¤§å¹¶å‘æ•°å¿…é¡»åœ¨1-20ä¹‹é—´")
        sys.exit(1)
    
    # é€‰æ‹©è¿è¡Œæ¨¡å¼
    if args.cli:
        run_cli(args)
    else:
        run_gui()

if __name__ == '__main__':
    main()